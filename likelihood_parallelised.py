import numpy as npimport pandas as pdfrom Q_matrix import Q_matrixfrom Ematrix import E_matrixfrom P_matrix import P_matrixfrom L_i_lt import L_i_ltfrom L_i import L_idef loglikelihood(*,p0,dt,covariates_name,no_qpars,no_epars,constrain,Qm,functions,age_like,E_covariates_name,ematrix,Left_trunc,death_states,censored_state ,corresponding_censored_states,lt_states,lt_cen_state, use_given_initial_prob,Use_misclass_matrix,initial_prob,**kwargs ):    p = p0[0:(no_qpars)]    p = p[constrain-1]    p_e=p0[max(constrain):(len(p0))]            if len(covariates_name)>0:        dt_q = dt.groupby(covariates_name).size().reset_index(name='counts')        qlength = len(dt_q.index)        dt_q['row'] = np.arange(dt_q.shape[0])    else:        dt_q = dt.assign(counts=len(dt))        qlength = 1        dt_q['row'] = 0    if len(covariates_name)>0:        Q_list = [Q_matrix(p,no_qpars,Qm,covariates_name,j,dt_q,functions,age_like) for j in range(0, qlength )]    else:        j=0        Q_list = [Q_matrix(p, no_qpars, Qm, covariates_name, j, dt_q, functions, age_like)]    dta_s = pd.merge(dt,dt_q,how = 'left')        dta_s=dta_s.sort_values(by=['id','age'])        if len(E_covariates_name)>0:        dt_E=dt.groupby(E_covariates_name).size().reset_index(name='counts')                dt_E['row_E'] = np.arange(dt_E.shape[0])                e_length = len(dt_E.index)                        E_list=[E_matrix(p_e,no_epars,ematrix,j,dt_E,E_covariates_name) for j in range(0, e_length)]                dta_s = pd.merge(dta_s,dt_E,how = 'left')    else:        dt_E = {'row_E':0  }        e_length = 1        E_list=[E_matrix(p_e,no_epars,ematrix,j,dt_E,E_covariates_name) for j in range(0, e_length)]        dta_s['row_E']=0            dta_s=dta_s.sort_values(by=['id','age'])            dta_p=dta_s    dta_p['lead_time']=dta_s.groupby(['id'])['age'].shift(-1)    dta_p['interval']=dta_p['lead_time']-dta_p['age']        dta_p=dta_p.fillna(0)            dt_p=dta_p.groupby(['row','interval']).size().reset_index(name='counts')        dt_p['row_p'] = np.arange(dt_p.shape[0])        plength = len(dt_p.index)            dta_p=dta_p.drop(columns=['counts'])    dta_p = pd.merge(dta_p,dt_p,how = 'left')    dta_p=dta_p.sort_values(by=['id','age'])        P_list=[P_matrix(Q_list,dt_p,j) for j in range(0,plength)]    if Left_trunc==True:        temp_dta = dt.copy()                # Group by 'id' and apply transformations        temp_dta['order_rank'] = temp_dta.groupby('id')['age'].rank()        temp_dta['min_age'] = temp_dta.groupby('id')['age'].transform('min')        temp_dta['age']=temp_dta['min_age']         temp_dta['state_1'] = temp_dta['state']                temp_dta = temp_dta[temp_dta['order_rank'] == 1]                temp_dta_group = temp_dta.copy()        temp_dta_group=temp_dta_group.groupby(['age','state']).size().reset_index(name='counts')                        dta_lt_age = list()                for i in temp_dta_group.index:                ltdta=temp_dta_group.loc[[i]].reset_index(drop=True)                                                seqs_time = np.arange(lt_assumption_age, ltdta['age'][0], grid_length)                seqs_time =np.append(seqs_time,ltdta['age'])                                                state_dta =np.repeat(lt_cen_state, len(seqs_time), axis=0)                state_dta[0]=1                state_dta[len(seqs_time)-1]=ltdta['state'][0]                                                    ltdta=ltdta.drop(columns=['counts'])                                                ltdta=ltdta.loc[ltdta.index.repeat(len(seqs_time)                )].reset_index(drop=True)                ltdta['state'] = state_dta                ltdta['age'] = seqs_time                if(len(age_like)>0):                    ltdta[age_like] = seqs_time                                                    ltdta['order_rank'] = 1                ltdta['min_age'] = max(seqs_time)                ltdta['state_1']  = state_dta[len(state_dta)-1]                dta_lt_age.append(ltdta)                                dta_lt_age=pd.concat(dta_lt_age).reset_index(drop=True)                         if len(age_like)>0:            temp_dta = temp_dta.drop(columns =['age','state',age_like  ] )        else:            temp_dta = temp_dta.drop(columns =['age','state'  ] )                dta_lt_temp = pd.merge(dta_lt_age,temp_dta,how = 'left')                dta_lt_temp = dta_lt_temp.drop(columns =['order_rank','state_1',"min_age"  ] )        if len(covariates_name) > 0:            dt_lt_q=dta_lt_temp.groupby(covariates_name).size().reset_index(name='counts')            qlength = len(dt_lt_q.index)            dt_lt_q['row'] = np.arange(dt_lt_q.shape[0])        else:            dt_lt_q = dt.assign(counts=len(dta_lt_temp))            qlength = 1            dt_lt_q['row'] = 0        Q_lt_list = [Q_matrix(p,no_qpars,Qm,covariates_name,j,dt_lt_q,functions,age_like) for j in range(0, qlength )]        dta_lt_s = pd.merge(dta_lt_temp,dt_lt_q,how = 'left')                dta_lt_s=dta_lt_s.sort_values(by=['id','age'])                        dta_lt_p=dta_lt_s        dta_lt_p['lead_time']=dta_lt_s.groupby(['id'])['age'].shift(-1)        dta_lt_p['interval']=dta_lt_p['lead_time']-dta_lt_p['age']                dta_lt_p=dta_lt_p.fillna(0)                dt_lt_p=dta_lt_p.groupby(['row','interval']).size().reset_index(name='counts')                dt_lt_p['row_p'] = np.arange(dt_lt_p.shape[0])                plength_lt = len(dt_lt_p.index)                        dta_lt_p=dta_lt_p.drop(columns=['counts'])        dta_lt_p = pd.merge(dta_lt_p,dt_lt_p,how = 'left')        dta_lt_p=dta_lt_p.sort_values(by=['id','age'])                P_lt_list=[P_matrix(Q_lt_list,dt_lt_p,j) for j in range(0,plength_lt)]    if Left_trunc==True:        # Pre-split to keep payloads small        groups_iter = dta_p.groupby("id", sort=False)        pairs = [(gid, gdf.reset_index(drop=True)) for gid, gdf in groups_iter]        def _call(gid, gdf):            return L_i_lt(gdf, gid, Q_list,P_list,death_states,censored_state ,corresponding_censored_states,E_list,dta_lt_p, Q_lt_list,P_lt_list,lt_states)        Li = Parallel(n_jobs=-1, backend="loky")(            delayed(_call)(gid, gdf) for gid, gdf in pairs)    else:        #if __name__ == "__main__":        #    Li = compute_Li_parallel_shared(        #        dta_p, Q_list, P_list,        #        death_states, censored_state, corresponding_censored_states,        #        E_list, use_given_initial_prob,        #        Use_misclass_matrix, p_e, no_epars, ematrix,        #        E_covariates_name, initial_prob,        #        processes=12  # or however many cores you want        #    )        Li = [L_i(dta_p, i, Q_list, P_list, death_states, censored_state, corresponding_censored_states, E_list,                  use_given_initial_prob, Use_misclass_matrix, p_e, no_epars, ematrix, E_covariates_name, initial_prob)              for i in pd.unique(dta_p['id'])]    Log_l = -sum(Li)    #print(p)    print(Log_l)    return Log_ldef objective_top(p, fixed):    return loglikelihood(p0=np.asarray(p), **fixed)