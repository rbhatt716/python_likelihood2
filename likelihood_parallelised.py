sys.path.append('/Users/rbhatt/Library/CloudStorage/OneDrive-Personal/python_likelihood2/python_likelihood2')import numpy as npimport pandas as pdfrom Q_matrix import Q_matrixfrom Ematrix import E_matrixfrom P_matrix import P_matrixfrom L_i_lt import L_i_ltfrom L_i import L_ifrom joblib import Parallel, delayedfrom optimparallel import minimize_paralleld = {'id':[1,1,1,2,2,2,3,3,3,4,4,4]   ,'age': [2,2.2,2.4, 3,3.2,3.4, 4,4.2,4.4, 2,2.2,2.5], 'x': [.1,.1,.1 ,  -.3,-.3,-.3,  -.4,-.4,-.4,   .1,.1,.1],'y': [1,1,1, 2,2,2, 3,3,3, 1,1,1],     'state':[1,2,3, 2,2,3, -1,1,2, 1,-1,2 ]  }dt = pd.read_csv("/Users/rbhatt/Library/CloudStorage/OneDrive-Personal/South Asian Migrant Project/processed_data.csv")p = [-3.17040926, -6.7375842 , -3.72734426 ,-1.88930381 ,-4.82858236]no_qpars = 5functions = ['exponential','exponential','exponential','exponential','exponential']age_like=Nonedeath_states = [4]censored_state = [-3,-2,-1]corresponding_censored_states = [[1,2],[2],[3]]lt_states = Noneconstrain = np.array([1,2,3,4,5 ])Qm = np.array([[0,.1,0, .1],      [0,0,.1,.1],      [0,0,0,.1],      [0,0,0,0]               ])covariates_name = []j=0Use_misclass_matrix=Falseuse_given_initial_prob=Falseinitial_prob = []Left_trunc=Falsep_e=[]no_epars=0E_covariates_name=[]ematrix = np.array([[0,0,0,0],           [0,0,0,0],           [0,0,0,0],           [0, 0, 0, 0]         ]           )grid_length = 1lt_assumption_age = Nonep0=np.append(p,p_e)dt = dt.rename(columns={'QID': 'id'})dt = dt.rename(columns={'State': 'state'})dt['id'] = dt['id'].astype(str)def loglikelihood(p0,dt,covariates_name,no_qpars,no_epars,constrain,Qm,functions,age_like,E_covariates_name,ematrix,Left_trunc,death_states,censored_state ,corresponding_censored_states,                  lt_states, use_given_initial_prob,Use_misclass_matrix,initial_prob ):        p = p0[0:(no_qpars)]    p = p[constrain-1]    p_e=p0[max(constrain):(len(p0))]            if len(covariates_name)>0:        dt_q = dt.groupby(covariates_name).size().reset_index(name='counts')        qlength = len(dt_q.index)        dt_q['row'] = np.arange(dt_q.shape[0])    else:        dt_q = dt.assign(counts=len(dt))        qlength = 1        dt_q['row'] = 0    if len(covariates_name)>0:        Q_list = [Q_matrix(p,no_qpars,Qm,covariates_name,j,dt_q,functions,age_like) for j in range(0, qlength )]    else:        j=0        Q_list = [Q_matrix(p, no_qpars, Qm, covariates_name, j, dt_q, functions, age_like)]    dta_s = pd.merge(dt,dt_q,how = 'left')        dta_s=dta_s.sort_values(by=['id','age'])        if len(E_covariates_name)>0:        dt_E=dt.groupby(E_covariates_name).size().reset_index(name='counts')                dt_E['row_E'] = np.arange(dt_E.shape[0])                e_length = len(dt_E.index)                        E_list=[E_matrix(p_e,no_epars,ematrix,j,dt_E,E_covariates_name) for j in range(0, e_length)]                dta_s = pd.merge(dta_s,dt_E,how = 'left')    else:        dt_E = {'row_E':0  }        e_length = 1        E_list=[E_matrix(p_e,no_epars,ematrix,j,dt_E,E_covariates_name) for j in range(0, e_length)]        dta_s['row_E']=0            dta_s=dta_s.sort_values(by=['id','age'])            dta_p=dta_s    dta_p['lead_time']=dta_s.groupby(['id'])['age'].shift(-1)    dta_p['interval']=dta_p['lead_time']-dta_p['age']        dta_p=dta_p.fillna(0)            dt_p=dta_p.groupby(['row','interval']).size().reset_index(name='counts')        dt_p['row_p'] = np.arange(dt_p.shape[0])        plength = len(dt_p.index)            dta_p=dta_p.drop(columns=['counts'])    dta_p = pd.merge(dta_p,dt_p,how = 'left')    dta_p=dta_p.sort_values(by=['id','age'])        P_list=[P_matrix(Q_list,dt_p,j) for j in range(0,plength)]            if Left_trunc==True:        temp_dta = dt.copy()                # Group by 'id' and apply transformations        temp_dta['order_rank'] = temp_dta.groupby('id')['age'].rank()        temp_dta['min_age'] = temp_dta.groupby('id')['age'].transform('min')        temp_dta['age']=temp_dta['min_age']         temp_dta['state_1'] = temp_dta['state']                temp_dta = temp_dta[temp_dta['order_rank'] == 1]                temp_dta_group = temp_dta.copy()        temp_dta_group=temp_dta_group.groupby(['age','state']).size().reset_index(name='counts')                        dta_lt_age = list()                for i in temp_dta_group.index:                ltdta=temp_dta_group.loc[[i]].reset_index(drop=True)                                                seqs_time = np.arange(lt_assumption_age, ltdta['age'][0], grid_length)                seqs_time =np.append(seqs_time,ltdta['age'])                                                state_dta =np.repeat(-1, len(seqs_time), axis=0)                state_dta[0]=1                state_dta[len(seqs_time)-1]=ltdta['state'][0]                                                    ltdta=ltdta.drop(columns=['counts'])                                                ltdta=ltdta.loc[ltdta.index.repeat(len(seqs_time)                )].reset_index(drop=True)                ltdta[age_like] = seqs_time                ltdta['state'] = state_dta                if(len(age_like)>0):                    ltdta[age_like] = seqs_time                                                    ltdta['order_rank'] = 1                ltdta['min_age'] = max(seqs_time)                ltdta['state_1']  = state_dta[len(state_dta)-1]                dta_lt_age.append(ltdta)                                dta_lt_age=pd.concat(dta_lt_age).reset_index(drop=True)                                 temp_dta = temp_dta.drop(columns =['age','state',age_like  ] )                dta_lt_temp = pd.merge(dta_lt_age,temp_dta,how = 'left')                dta_lt_temp = dta_lt_temp.drop(columns =['order_rank','state_1',"min_age"  ] )                dt_lt_q=dta_lt_temp.groupby(covariates_name).size().reset_index(name='counts')        qlength = len(dt_lt_q.index)                dt_lt_q['row'] = np.arange(dt_lt_q.shape[0])                Q_lt_list = [Q_matrix(p,no_qpars,Qm,covariates_name,j,dt_lt_q,functions,age_like) for j in range(0, qlength )]        dta_lt_s = pd.merge(dta_lt_temp,dt_lt_q,how = 'left')                dta_lt_s=dta_lt_s.sort_values(by=['id','age'])                dta_lt_s=dta_lt_s.sort_values(by=['id','age'])                        dta_lt_p=dta_lt_s        dta_lt_p['lead_time']=dta_lt_s.groupby(['id'])['age'].shift(-1)        dta_lt_p['interval']=dta_lt_p['lead_time']-dta_lt_p['age']                dta_lt_p=dta_lt_p.fillna(0)                dt_lt_p=dta_lt_p.groupby(['row','interval']).size().reset_index(name='counts')                dt_lt_p['row_p'] = np.arange(dt_lt_p.shape[0])                plength_lt = len(dt_lt_p.index)                        dta_lt_p=dta_lt_p.drop(columns=['counts'])        dta_lt_p = pd.merge(dta_lt_p,dt_lt_p,how = 'left')        dta_lt_p=dta_lt_p.sort_values(by=['id','age'])                P_lt_list=[P_matrix(Q_lt_list,dt_lt_p,j) for j in range(0,plength_lt)]    if Left_trunc==True:        # Pre-split to keep payloads small        groups_iter = dta_p.groupby("id", sort=False)        pairs = [(gid, gdf.reset_index(drop=True)) for gid, gdf in groups_iter]        def _call(gid, gdf):            return L_i_lt(gdf, gid, Q_list,P_list,death_states,censored_state ,corresponding_censored_states,E_list,dta_lt_p, Q_lt_list,P_lt_list,lt_states)        Li = Parallel(n_jobs=-1, backend="loky")(            delayed(_call)(gid, gdf) for gid, gdf in pairs)    else:        # Pre-split to keep payloads small        groups_iter = dta_p.groupby("id", sort=False)        pairs = [(gid, gdf.reset_index(drop=True)) for gid, gdf in groups_iter]        def _call(gid, gdf):            return L_i(gdf, gid, Q_list, P_list, death_states, censored_state,                       corresponding_censored_states, E_list, use_given_initial_prob,                       Use_misclass_matrix, p_e, no_epars, ematrix, E_covariates_name, initial_prob)        Li = Parallel(n_jobs=-1, backend="loky")(            delayed(_call)(gid, gdf) for gid, gdf in pairs)    Log_l = -sum(Li)    print(p)    print(Log_l)    return Log_l   import timestart=time.time()loglikelihood(p0,dt,covariates_name,no_qpars,no_epars,constrain,Qm,functions,age_like,E_covariates_name,ematrix,Left_trunc,death_states,censored_state ,corresponding_censored_states,      lt_states, use_given_initial_prob,Use_misclass_matrix,initial_prob )end=time.time()from scipy.optimize import minimizefrom functools import partialobjective = lambda p0: loglikelihood(    p0, dt, covariates_name, no_qpars, no_epars, constrain, Qm,    functions, age_like, E_covariates_name, ematrix,    Left_trunc, death_states, censored_state, corresponding_censored_states,    lt_states, use_given_initial_prob, Use_misclass_matrix, initial_prob)result = minimize(    objective,    x0=p0,    method='BFGS',  # or 'L-BFGS-B', 'Nelder-Mead', etc.    options={'disp': True})result